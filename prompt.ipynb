{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-03-25T04:57:30.3149164Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2066186300, 'load_duration': 1339647600, 'prompt_eval_count': 32, 'prompt_eval_duration': 359090100, 'eval_count': 8, 'eval_duration': 366942600, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-cc12d135-33ab-4e32-a3d9-c752058da179-0', usage_metadata={'input_tokens': 32, 'output_tokens': 8, 'total_tokens': 40})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "\n",
    "llm.invoke(\"What is the capital of France?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='What is the capital of France?'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-03-25T04:58:36.6158689Z', 'done': True, 'done_reason': 'stop', 'total_duration': 401979100, 'load_duration': 18068700, 'prompt_eval_count': 32, 'prompt_eval_duration': 48317700, 'eval_count': 8, 'eval_duration': 335592700, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-56759e8b-6c03-4232-9465-943bfa32e66e-0', usage_metadata={'input_tokens': 32, 'output_tokens': 8, 'total_tokens': 40})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"What is the capital of {country}?\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.invoke({\"country\": \"France\"})\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "llm.invoke(prompt_template.invoke({\"country\": \"France\"}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Literal, Union\n",
    "\n",
    "from langchain_core.messages import HumanMessage, BaseMessage\n",
    "\n",
    "class CustomMessage(BaseMessage):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of Spain is Madrid.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-03-25T05:19:17.7771206Z', 'done': True, 'done_reason': 'stop', 'total_duration': 400879900, 'load_duration': 19056700, 'prompt_eval_count': 86, 'prompt_eval_duration': 55809000, 'eval_count': 8, 'eval_duration': 324355100, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-add2dd39-60c7-45f0-a28f-a421547ea133-0', usage_metadata={'input_tokens': 86, 'output_tokens': 8, 'total_tokens': 94})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "message_list = [ # shot: 답변 형식에 대한 예제 (마치 대화 이력이 있는 것처럼 llm을 속여서 답변을 유도)\n",
    "    SystemMessage(content=\"You are a helpful assistant!\"),\n",
    "    HumanMessage(content=\"What is the capital of France?\"),\n",
    "    AIMessage(content=\"The capital of France is Paris.\"),\n",
    "    HumanMessage(content=\"What is the capital of Germany?\"),\n",
    "    AIMessage(content=\"The capital of Germany is Berlin.\"),\n",
    "    HumanMessage(content=\"What is the capital of Spain?\")\n",
    "]\n",
    "\n",
    "llm.invoke(message_list)\n",
    "\n",
    "# 복잡한 어플리케이션을 구현할 때 위와 같이 예제를 주는게 정확도를 높이는 효과가 있음\n",
    "# 하지만 langchain스럽지 않다. \n",
    "# 왜냐하면 langchain에는 chat prompt template가 있기 때문이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful assistant!', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant!\"),\n",
    "    (\"human\", \"What is the capital of {country}?\"),\n",
    "])\n",
    "\n",
    "chat_prompt = chat_prompt_template.invoke({\"country\": \"France\"})\n",
    "\n",
    "print(chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-03-25T05:28:22.1371883Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2138719200, 'load_duration': 1381958400, 'prompt_eval_count': 38, 'prompt_eval_duration': 416597200, 'eval_count': 8, 'eval_duration': 339140900, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-16eff757-6e57-4093-99ac-05f2d32cfda8-0', usage_metadata={'input_tokens': 38, 'output_tokens': 8, 'total_tokens': 46})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대화 이력을 퓨샷으로 넣어줄 때는 \n",
    "# 메시지 리스트를 사용하는 것보다 \n",
    "# ChatPromtpTemplate를 사용하는게 langchin스러운것 같다. \n",
    "\n",
    "# ChatPromtpTemplate는 추후에 LCEL 관련 연동이 되는데, \n",
    "# 메시지 리스트는 연동이 안된다.\n",
    "# 그래서 ChatPromtpTemplate를 사용하는게 확장성에서 좋다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
